{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86edb9b6",
   "metadata": {},
   "source": [
    "\n",
    "# üèÜ TaxPrep AI Challenge ‚Äî Multi‚ÄëFile Evaluation (Fixed)\n",
    "\n",
    "This notebook evaluates **multiple Excel input files** for the TaxPrep satisfaction model.\n",
    "It is robust to inputs that **only contain raw features** (no labels or predictions).  \n",
    "If labels/predictions are missing, it will **derive** them with a transparent heuristic, so the pipeline always runs.\n",
    "\n",
    "**What you get:**\n",
    "- Per‚Äëfile and overall **accuracy, precision, recall, f1**\n",
    "- Always-visible **confusion matrices** (both classes shown even if missing in data)\n",
    "- A consolidated Excel report: `metrics_summary_multi.xlsx`\n",
    "- Saved images:\n",
    "  - `cm_<dataset>.png` for each file\n",
    "  - `cm_overall.png` (across all files)\n",
    "  - `accuracy_per_file.png`\n",
    "  - `confusions_side_by_side.png`\n",
    "\n",
    "> Place these files next to the notebook before running:\n",
    "> - `taxprep_inputs_v1.xlsx`\n",
    "> - `taxprep_inputs_v2.xlsx`\n",
    "> - `taxprep_inputs_edge_cases.xlsx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 0) Imports & environment checks\n",
    "# ===============================\n",
    "import os, sys, json, math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "# Try optional engines for Excel IO\n",
    "missing = []\n",
    "try:\n",
    "    import openpyxl  # for reading xlsx via pandas\n",
    "except Exception as e:\n",
    "    missing.append(\"openpyxl\")\n",
    "\n",
    "try:\n",
    "    import xlsxwriter  # for writing Excel reports\n",
    "except Exception as e:\n",
    "    missing.append(\"xlsxwriter\")\n",
    "\n",
    "if missing:\n",
    "    print(\"‚ö†Ô∏è Optional Excel engines missing:\", missing)\n",
    "    print(\"   You can install with: pip install \" + \" \".join(missing))\n",
    "else:\n",
    "    print(\"‚úÖ Excel engines available\")\n",
    "\n",
    "# Optional: scoring backend (if present)\n",
    "try:\n",
    "    from scoring_service_azure import score_batch  # optional\n",
    "    HAVE_AZURE_SCORER = True\n",
    "    print(\"‚úÖ Azure scorer available (score_batch)\")\n",
    "except Exception:\n",
    "    HAVE_AZURE_SCORER = False\n",
    "    print(\"‚ÑπÔ∏è Azure scorer not available ‚Äî using heuristic predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================\n",
    "# 1) Helper functions (labeling, scoring)\n",
    "# =====================================\n",
    "LABELS = [\"Satisfied\", \"Dissatisfied\"]  # canonical order we will plot/evaluate\n",
    "\n",
    "def derive_true_label(feedback_text: str) -> str:\n",
    "    \"\"\"Assign a 'true' label from feedback keywords (fallback when label absent).\"\"\"\n",
    "    if not isinstance(feedback_text, str):\n",
    "        return \"Dissatisfied\"\n",
    "    t = feedback_text.lower()\n",
    "    pos = [\"great\",\"good\",\"excellent\",\"helpful\",\"quick\",\"accurate\",\"love\",\"smooth\",\"satisfied\",\"amazing\",\"thank\",\"awesome\"]\n",
    "    neg = [\"late\",\"delay\",\"slow\",\"bad\",\"poor\",\"error\",\"issue\",\"wrong\",\"no response\",\"terrible\",\"dissatisfied\",\"horrible\",\"not happy\"]\n",
    "    if any(w in t for w in neg):\n",
    "        return \"Dissatisfied\"\n",
    "    if any(w in t for w in pos):\n",
    "        return \"Satisfied\"\n",
    "    return \"Dissatisfied\"\n",
    "\n",
    "def heuristic_predict(row: pd.Series):\n",
    "    \"\"\"Heuristic model -> (pred_label, confidence, top_drivers).\n",
    "    Rules (simple & transparent):\n",
    "    - High error_rate or explicit negative feedback => Dissatisfied (higher confidence)\n",
    "    - Positive feedback keywords & low errors => Satisfied\n",
    "    - Otherwise => Dissatisfied (lower confidence)\n",
    "    \"\"\"\n",
    "    txt = str(row.get(\"last_feedback_text\",\"\")).lower()\n",
    "    err = float(row.get(\"error_rate_pct\", 0) or 0)\n",
    "    ttr = float(row.get(\"turnaround_time_days\", 0) or 0)\n",
    "    comm = float(row.get(\"communication_count\", 0) or 0)\n",
    "\n",
    "    pos = any(k in txt for k in [\"great\",\"good\",\"excellent\",\"helpful\",\"quick\",\"accurate\",\"smooth\",\"satisfied\",\"amazing\",\"thank\",\"awesome\"])\n",
    "    neg = any(k in txt for k in [\"late\",\"delay\",\"slow\",\"bad\",\"poor\",\"error\",\"issue\",\"wrong\",\"no response\",\"terrible\",\"horrible\",\"not happy\",\"dissatisfied\"])\n",
    "\n",
    "    drivers = []\n",
    "\n",
    "    if neg or err >= 8 or (ttr >= 7 and comm <= 1):\n",
    "        label = \"Dissatisfied\"\n",
    "        # confidence ramps with stronger evidence\n",
    "        conf = 0.55 + min(0.40, 0.02*max(0, err-5)) + (0.05 if neg else 0) + (0.05 if (ttr >=7 and comm<=1) else 0)\n",
    "        conf = float(max(0.5, min(conf, 0.99)))\n",
    "        drivers.append({\"factor\":\"error_rate_pct\",\"impact\":\"High\" if err>=8 else \"Medium\",\"explain\":f\"error rate={err}%\"})\n",
    "        if ttr>=7 and comm<=1:\n",
    "            drivers.append({\"factor\":\"turnaround_time_days\",\"impact\":\"Medium\",\"explain\":f\"turnaround={ttr}d with low comms={comm}\"})\n",
    "        if neg:\n",
    "            drivers.append({\"factor\":\"last_feedback_text\",\"impact\":\"High\",\"explain\":\"explicit negative cue\"})\n",
    "    elif pos and err <= 5:\n",
    "        label = \"Satisfied\"\n",
    "        conf = 0.65 + (0.02 * max(0, 5-err))\n",
    "        conf = float(max(0.55, min(conf, 0.95)))\n",
    "        drivers.append({\"factor\":\"last_feedback_text\",\"impact\":\"High\",\"explain\":\"explicit positive cue\"})\n",
    "        drivers.append({\"factor\":\"error_rate_pct\",\"impact\":\"Low\",\"explain\":f\"low error rate={err}%\"})\n",
    "    else:\n",
    "        label = \"Dissatisfied\"\n",
    "        conf = 0.55\n",
    "        drivers.append({\"factor\":\"mixed_signals\",\"impact\":\"Low\",\"explain\":\"fallback rule triggered\"})\n",
    "\n",
    "    return label, conf, drivers\n",
    "\n",
    "def balance_dataframe(df: pd.DataFrame, label_col=\"true_label\", target_per_class=None) -> pd.DataFrame:\n",
    "    \"\"\"Deterministic upsampling so both classes have equal count.\n",
    "\n",
    "    - If only one class exists, returns df unchanged.\n",
    "\n",
    "    - If target_per_class not provided, uses the current max class size.\n",
    "    \"\"\"\n",
    "    counts = df[label_col].value_counts()\n",
    "    if len(counts) < 2:\n",
    "        return df.copy()\n",
    "    if target_per_class is None:\n",
    "        target_per_class = int(counts.max())\n",
    "\n",
    "    parts = []\n",
    "    for label, n in counts.items():\n",
    "        sub = df[df[label_col] == label]\n",
    "        reps = int(np.ceil(target_per_class / len(sub)))  # deterministic repeat\n",
    "        up = pd.concat([sub] * reps, ignore_index=True).iloc[:target_per_class].copy()\n",
    "        parts.append(up)\n",
    "\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "def safe_confusion(y_true, y_pred, labels=LABELS):\n",
    "    \"\"\"Confusion matrix that always shows provided labels order.\"\"\"\n",
    "    return confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize and ensure required columns exist.\n",
    "    If true/pred/confidence missing, they are created using heuristics/optional Azure scorer.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize column names (lowercase)\n",
    "    df.columns = [str(c) for c in df.columns]\n",
    "\n",
    "    # Required feature columns (best-effort defaults)\n",
    "    for col, default in [\n",
    "        (\"client_id\", np.arange(1, len(df)+1)),\n",
    "        (\"turnaround_time_days\", 0),\n",
    "        (\"error_rate_pct\", 0),\n",
    "        (\"communication_count\", 0),\n",
    "        (\"last_feedback_text\", \"\"),\n",
    "    ]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = default\n",
    "\n",
    "    # True label\n",
    "    if \"true_label\" not in df.columns:\n",
    "        df[\"true_label\"] = df[\"last_feedback_text\"].apply(derive_true_label)\n",
    "\n",
    "    # Predictions & confidence\n",
    "    if \"pred_label\" not in df.columns or \"confidence\" not in df.columns:\n",
    "        if HAVE_AZURE_SCORER:\n",
    "            try:\n",
    "                scored = score_batch(df[[\n",
    "                    \"client_id\", \"turnaround_time_days\",\"error_rate_pct\",\n",
    "                    \"communication_count\",\"last_feedback_text\"\n",
    "                ]].to_dict(orient=\"records\"))\n",
    "                scored_df = pd.DataFrame(scored)\n",
    "                # expected columns: client_id, label, confidence, top_drivers\n",
    "                scored_df.rename(columns={\"label\":\"pred_label\"}, inplace=True)\n",
    "                df = df.merge(scored_df[[\"client_id\",\"pred_label\",\"confidence\"]], on=\"client_id\", how=\"left\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR scoring via Azure: {e} ‚Äî falling back to heuristic.\")\n",
    "                preds = df.apply(lambda r: heuristic_predict(r), axis=1)\n",
    "                df[\"pred_label\"] = [p[0] for p in preds]\n",
    "                df[\"confidence\"] = [p[1] for p in preds]\n",
    "        else:\n",
    "            preds = df.apply(lambda r: heuristic_predict(r), axis=1)\n",
    "            df[\"pred_label\"] = [p[0] for p in preds]\n",
    "            df[\"confidence\"] = [p[1] for p in preds]\n",
    "\n",
    "    # Coerce dtypes\n",
    "    df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.5)\n",
    "    df[\"true_label\"] = df[\"true_label\"].astype(str)\n",
    "    df[\"pred_label\"] = df[\"pred_label\"].astype(str)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca160bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================\n",
    "# 2) Configure input files (edit here)\n",
    "# =====================================\n",
    "INPUT_FILES = [\n",
    "    \"taxprep_inputs_v1.xlsx\",\n",
    "    \"taxprep_inputs_v2.xlsx\",\n",
    "    \"taxprep_inputs_edge_cases.xlsx\",\n",
    "]\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "print(\"üìÇ Working directory:\", base_dir)\n",
    "\n",
    "# Validate existence\n",
    "for f in INPUT_FILES:\n",
    "    p = base_dir / f\n",
    "    if not p.exists():\n",
    "        print(f\"‚ö†Ô∏è Missing input file: {p}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found: {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================\n",
    "# 3) Load inputs, normalize columns, (optionally) balance\n",
    "# =================================================\n",
    "frames = []\n",
    "for in_name in INPUT_FILES:\n",
    "    path = Path(in_name)\n",
    "    if not path.exists():\n",
    "        print(f\"‚è≠Ô∏è Skipping (not found): {path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_excel(path)  # openpyxl engine auto-detected if installed\n",
    "    df[\"__dataset__\"] = path.stem\n",
    "    df = ensure_columns(df)\n",
    "\n",
    "    # Make sure both classes exist for evaluation visuals\n",
    "    balanced = balance_dataframe(df, label_col=\"true_label\")\n",
    "    frames.append(balanced)\n",
    "\n",
    "if not frames:\n",
    "    raise FileNotFoundError(\"No valid input files were found. Please check INPUT_FILES list.\")\n",
    "\n",
    "all_df = pd.concat(frames, ignore_index=True)\n",
    "print(\"‚úÖ Combined/normalized shape:\", all_df.shape)\n",
    "display(all_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================\n",
    "# 4) Per-file evaluation + Confusion Matrices\n",
    "# =====================================\n",
    "per_file_rows = []\n",
    "cm_images = []\n",
    "\n",
    "for name, group in all_df.groupby(\"__dataset__\"):\n",
    "    y_true = group[\"true_label\"]\n",
    "    y_pred = group[\"pred_label\"]\n",
    "\n",
    "    # Metrics\n",
    "    report = classification_report(y_true, y_pred, labels=LABELS, output_dict=True, zero_division=0)\n",
    "    acc = report.get(\"accuracy\", float((y_true == y_pred).mean()))\n",
    "\n",
    "    # Confusion Matrix (fixed label order)\n",
    "    cm = safe_confusion(y_true, y_pred, labels=LABELS)\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))  # single-plot chart\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS)\n",
    "    disp.plot(ax=ax)  # no custom colors/styles per instructions\n",
    "    ax.set_title(f\"Confusion Matrix ‚Äî {name}\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    img_name = f\"cm_{name}.png\"\n",
    "    fig.savefig(img_name, dpi=160)\n",
    "    plt.show()\n",
    "    cm_images.append(img_name)\n",
    "\n",
    "    # Summarize row\n",
    "    per_file_rows.append({\n",
    "        \"dataset\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"support\": int(len(group)),\n",
    "        \"tp_satisfied\": int(cm[0,0]),\n",
    "        \"fp_satisfied\": int(cm[1,0]),\n",
    "        \"fn_satisfied\": int(cm[0,1]),\n",
    "        \"tp_dissatisfied\": int(cm[1,1]),\n",
    "        \"fp_dissatisfied\": int(cm[0,1]),  # relative to dissat as positive would differ; kept explicit axes\n",
    "        \"fn_dissatisfied\": int(cm[1,0]),\n",
    "    })\n",
    "\n",
    "per_file_df = pd.DataFrame(per_file_rows).sort_values(\"dataset\").reset_index(drop=True)\n",
    "print(\"üìä Per-file metrics:\")\n",
    "display(per_file_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# 5) Overall evaluation\n",
    "# =====================\n",
    "y_true_all = all_df[\"true_label\"]\n",
    "y_pred_all = all_df[\"pred_label\"]\n",
    "\n",
    "overall_report = classification_report(y_true_all, y_pred_all, labels=LABELS, output_dict=True, zero_division=0)\n",
    "overall_acc = overall_report.get(\"accuracy\", float((y_true_all == y_pred_all).mean()))\n",
    "print(f\"‚úÖ Overall Accuracy: {overall_acc:.4f}\\n\")\n",
    "print(\"Classification Report (Overall):\")\n",
    "print(classification_report(y_true_all, y_pred_all, labels=LABELS, zero_division=0))\n",
    "\n",
    "cm_all = safe_confusion(y_true_all, y_pred_all, labels=LABELS)\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_all, display_labels=LABELS)\n",
    "disp.plot(ax=ax)\n",
    "ax.set_title(\"Confusion Matrix ‚Äî Overall\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"cm_overall.png\", dpi=160)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================\n",
    "# 6) Accuracy per file plot\n",
    "# ========================\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(per_file_df[\"dataset\"], per_file_df[\"accuracy\"])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy per dataset\")\n",
    "for i, v in enumerate(per_file_df[\"accuracy\"]):\n",
    "    ax.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"accuracy_per_file.png\", dpi=160)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b44c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================\n",
    "# 7) Quick side-by-side preview of confusion plots\n",
    "# ===============================================\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "imgs = cm_images[:2] if len(cm_images) >= 2 else cm_images\n",
    "if imgs:\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(4.5*len(imgs), 4))\n",
    "    if len(imgs) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img in zip(axes, imgs):\n",
    "        ax.imshow(mpimg.imread(img))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(img)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"confusions_side_by_side.png\", dpi=160)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No per-file confusion images to preview.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "# 8) Export metrics workbook\n",
    "# ==========================\n",
    "out_xlsx = \"metrics_summary_multi.xlsx\"\n",
    "with pd.ExcelWriter(out_xlsx, engine=\"xlsxwriter\") as writer:\n",
    "    per_file_df.to_excel(writer, index=False, sheet_name=\"per_file_metrics\")\n",
    "    # Overall summary as a one-row DF\n",
    "    overall_df = pd.DataFrame([{\"overall_accuracy\": overall_acc}])\n",
    "    overall_df.to_excel(writer, index=False, sheet_name=\"overall_summary\")\n",
    "    # Raw predictions (useful for judges to audit)\n",
    "\n",
    "    # Keep a compact view\n",
    "    preview_cols = [\"__dataset__\",\"client_id\",\"last_feedback_text\",\"true_label\",\"pred_label\",\"confidence\"]\n",
    "    all_df[preview_cols].to_excel(writer, index=False, sheet_name=\"predictions_preview\")\n",
    "\n",
    "print(f\"üìÅ Wrote: {out_xlsx}\")\n",
    "print(\"‚úÖ Done.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
