{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488caa53",
   "metadata": {},
   "source": [
    "# TAXPREP_DEMO ‚Äî Scoring & Evaluation (Updated)\n",
    "This notebook:\n",
    "- Uses your improved `derive_true_label`\n",
    "- Balances dataset properly for both classes\n",
    "- Calls `scoring_service_azure` safely with fallback\n",
    "- Displays full evaluation + confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b3fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Setup & Imports\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure project root is in path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Optional: load environment vars\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "print(\"‚úÖ Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ddf9e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not import scoring_service_azure: ModuleNotFoundError No module named 'dotenv'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2Ô∏è‚É£ Import scoring module\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscoring_service_azure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m score_batch, balance_dataframe, derive_true_label\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ scoring_service_azure module loaded successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIML_Pravartak/TAXPREP_DEMO/taxprep_demo1/scoring_service_azure.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtenacity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m retry, stop_after_attempt, wait_exponential\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resample\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Logging setup\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------------------\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Import scoring module\n",
    "try:\n",
    "    from scoring_service_azure import score_batch, balance_dataframe, derive_true_label\n",
    "    print(\"‚úÖ scoring_service_azure module loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Could not import scoring_service_azure:\", type(e).__name__, e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Example data\n",
    "data = [\n",
    "    {\"client_id\": 1, \"turnaround_time_days\": 8, \"error_rate_pct\": 12, \"communication_count\": 1, \"last_feedback_text\": \"Late delivery\"},\n",
    "    {\"client_id\": 2, \"turnaround_time_days\": 5, \"error_rate_pct\": 2, \"communication_count\": 4, \"last_feedback_text\": \"Great service\"},\n",
    "    {\"client_id\": 3, \"turnaround_time_days\": 9, \"error_rate_pct\": 10, \"communication_count\": 0, \"last_feedback_text\": \"No response\"},\n",
    "    {\"client_id\": 4, \"turnaround_time_days\": 4, \"error_rate_pct\": 1, \"communication_count\": 5, \"last_feedback_text\": \"Helpful advisor\"},\n",
    "    {\"client_id\": 5, \"turnaround_time_days\": 2, \"error_rate_pct\": 0.5, \"communication_count\": 6, \"last_feedback_text\": \"Quick and accurate filing\"},\n",
    "    {\"client_id\": 6, \"turnaround_time_days\": 10, \"error_rate_pct\": 15, \"communication_count\": 0, \"last_feedback_text\": \"Terrible support\"},\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "print(\"‚úÖ Loaded data:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Derive true labels using improved derive_true_label\n",
    "df[\"true_label\"] = df[\"last_feedback_text\"].apply(derive_true_label)\n",
    "print(\"‚úÖ Derived true labels:\")\n",
    "display(df[[\"client_id\", \"last_feedback_text\", \"true_label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ Balance dataset (ensures both classes)\n",
    "balanced = balance_dataframe(df.copy())\n",
    "if balanced[\"true_label\"].nunique() < 2:\n",
    "    print(\"‚ö†Ô∏è Only one class found ‚Äî synthesizing balanced dataset.\")\n",
    "    satisfied_row = df.iloc[1].copy()\n",
    "    satisfied_row[\"client_id\"] = 999\n",
    "    satisfied_row[\"last_feedback_text\"] = \"Excellent and helpful service\"\n",
    "    satisfied_row[\"true_label\"] = \"Satisfied\"\n",
    "    balanced = pd.concat([df, pd.DataFrame([satisfied_row])], ignore_index=True)\n",
    "\n",
    "print(\"‚úÖ Balanced dataset class counts:\")\n",
    "print(balanced[\"true_label\"].value_counts())\n",
    "display(balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6Ô∏è‚É£ Score dataset via Azure or fallback\n",
    "print(\"üöÄ Scoring dataset via Azure OpenAI (or fallback)...\")\n",
    "results = score_batch(balanced)\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"‚úÖ Scoring complete:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a10bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7Ô∏è‚É£ Merge & Evaluate\n",
    "merged = balanced.merge(results_df, on=\"client_id\", how=\"left\").rename(columns={\"label\":\"pred_label\"})\n",
    "print(\"üîç Merged results:\")\n",
    "display(merged[[\"client_id\", \"true_label\", \"pred_label\", \"confidence\"]])\n",
    "\n",
    "y_true = merged[\"true_label\"]\n",
    "y_pred = merged[\"pred_label\"].fillna(\"Dissatisfied\")\n",
    "\n",
    "labels = [\"Dissatisfied\", \"Satisfied\"]\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "print(f\"\\nüìä Evaluation Metrics\\n‚úÖ Accuracy: {accuracy:.3f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=labels))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix (Both Classes)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
